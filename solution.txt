Q1, part1:

for tolerance = 0.001, value functions is:

[-5.93335500466204, -1.2163043094920325, 7.402984453095129, 50.0]
[-8.846649352267068, 0, -18.840604765787226, -50.0]
[-11.250142576036916, -13.600961387283313, -17.854554899641037, -26.561530258882197]
[-11.86063409384196, -13.30017974945419, -15.906691286347932, -19.191278729241084]


Q1, part2: 

optimal policy is:
        U = (-1, 0)
        D = (1, 0)
        R = (0, 1)
        L = (0, -1)

state: (0, 0), actions: [R]
state: (0, 1), actions: [R]
state: (0, 2), actions: [R]
state: (0, 3), actions: [R, L, D, U] # This happens because I have set transition probability to zero for Goal and Trap
state: (1, 0), actions: [U]
state: (1, 2), actions: [U]
state: (1, 3), actions: [R, L, D, U] # This happens because I have set transition probability to zero for Goal and Trap
state: (2, 0), actions: [U]
state: (2, 1), actions: [L]
state: (2, 2), actions: [U]
state: (2, 3), actions: [D]
state: (3, 0), actions: [U]
state: (3, 1), actions: [U]
state: (3, 2), actions: [U]
state: (3, 3), actions: [L]

for tolerance = 0.001, value function is:
[25.015276075875516, 31.60111866911516, 38.204470913323824, 50.0]
[20.218647908826032, 0, 18.865684016430194, -50.0]
[15.354140107963886, 11.434309555756636, 13.199765776810228, 5.7098674683271105]
[11.43430955575802, 9.01949548070337, 9.424301336148043, 6.59895895686007]
